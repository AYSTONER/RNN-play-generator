{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AYSTONER/RNN-play-generator/blob/main/RNN_play_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9Z7crrt3M902"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cs2ZaA2yOpO7"
      },
      "outputs": [],
      "source": [
        "# saving the path to it\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YHRf2eREOqPH"
      },
      "outputs": [],
      "source": [
        "# to import your own file, use this code\n",
        "# from google.colab import files\n",
        "# path_to_file = list(files.upload().keys())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2vJO3HlPoBC",
        "outputId": "0f570bf7-958c-47db-fab7-e47f010079b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# read the file then decode\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# len of text is the amount of characters in it\n",
        "print(f'Length of text: {len(text)} characters')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5UOqH9RUQmfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe045f5-9361-400e-9320-50bd22c0ad60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# first 250 characyers\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CjRRkN-YRU24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0af0aaf-1a94-44a9-a4fc-bf51d27c4a2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ],
      "source": [
        "# sort the unique characters in the data\n",
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "text_as_int=text_to_int(text)\n",
        "\n",
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nEBiX0k-WTYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1afb511a-b502-40ba-dad4-c1d8307578c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ],
      "source": [
        "def int_to_text(ints):\n",
        "\n",
        "  try:\n",
        "\n",
        "    ints = ints.numpy()\n",
        "\n",
        "  except:\n",
        "\n",
        "    pass\n",
        "\n",
        "  return \"\".join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "w7fg7MU4W1mw"
      },
      "outputs": [],
      "source": [
        "# creating a training data\n",
        "seq_length = 100 #length of sequence for a training exmple\n",
        "num_per_epoch = len(text) // (seq_length + 1)\n",
        "# this code converts the entire string dataset into characters and will contain a stream of characters(training examples)\n",
        "character_dst = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XcrM3zqjZKWl"
      },
      "outputs": [],
      "source": [
        "# use the batch method to batch the characters\n",
        "sequences = character_dst.batch(seq_length+1, drop_remainder=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rpxddYiwo-wi"
      },
      "outputs": [],
      "source": [
        "# use the sequence of length 101 and split into input and output\n",
        "def split_input_target(chunk): # for the example: hello\n",
        "  input_text = chunk[:-1] #   hell\n",
        "  target_text = chunk [1:] # ello\n",
        "  return input_text, target_text # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target) #we use map to apply the above function to every entry\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_2wNW54Kve38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e83d0af-5978-4d55-a4af-224d62d0fe60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n"
          ]
        }
      ],
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(x))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create our training batches\n",
        "BATCH_SIZE =64\n",
        "VOCAB_SIZE = len(vocab)#number of unique characters\n",
        "EMBDDING_DIM = 256\n",
        "RNN_UNIT = 1024\n",
        "\n",
        "BUFFER_SIZE = 1000\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder= True)\n"
      ],
      "metadata": {
        "id": "1JxaM8zdITFs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# building the model\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                         recurrent_initializer='glorot_uniform'),\n",
        "  tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBDDING_DIM, RNN_UNIT, BATCH_SIZE)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkAliXgRIWJg",
        "outputId": "4e21237c-4fd9-4d5f-b0c0-32452192c9c1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5330241 (20.33 MB)\n",
            "Trainable params: 5330241 (20.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a loss function\n",
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch) #ask our model for a prediction on our first batch of training data\n",
        "  print(example_batch_predictions.shape,\" (batch_size, sequence_length, vocab_size)\") #print out the output shape\n",
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Azi5JbR6IXCD",
        "outputId": "d092a24c-ed41-4b50-d870-7280fc374289"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65)  (batch_size, sequence_length, vocab_size)\n",
            "64\n",
            "tf.Tensor(\n",
            "[[[-0.00339666  0.00372586 -0.00232849 ... -0.00115589 -0.00019825\n",
            "    0.00094499]\n",
            "  [-0.00316992  0.00132619 -0.00286747 ... -0.00390963 -0.00041807\n",
            "   -0.0013145 ]\n",
            "  [-0.00281235  0.00308569 -0.00754597 ... -0.00070454  0.00506456\n",
            "   -0.00220535]\n",
            "  ...\n",
            "  [-0.00888389 -0.00183344 -0.00569069 ... -0.00555229  0.00339345\n",
            "   -0.00232633]\n",
            "  [-0.00691008 -0.0031834  -0.00688785 ... -0.00659937  0.00247563\n",
            "   -0.00440373]\n",
            "  [-0.00314979 -0.00036527 -0.00271043 ...  0.00134703  0.00321952\n",
            "   -0.00168075]]\n",
            "\n",
            " [[ 0.00354769 -0.00012214 -0.00555308 ...  0.00184843  0.00082338\n",
            "   -0.00334246]\n",
            "  [ 0.00151818 -0.00017169 -0.0076186  ... -0.00167721  0.00033441\n",
            "    0.00298148]\n",
            "  [-0.00112527  0.0031728  -0.01152877 ...  0.00115662 -0.00094682\n",
            "    0.00086874]\n",
            "  ...\n",
            "  [ 0.00126767 -0.00155362 -0.00449295 ... -0.0085513   0.00485331\n",
            "    0.001348  ]\n",
            "  [ 0.00188477 -0.003213   -0.00571553 ... -0.00752354  0.0045382\n",
            "   -0.00038649]\n",
            "  [ 0.00286737 -0.00049274 -0.00900581 ... -0.0078552   0.00527513\n",
            "    0.00436585]]\n",
            "\n",
            " [[ 0.00391738  0.00324448 -0.00064965 ... -0.00490717  0.00116425\n",
            "   -0.00050598]\n",
            "  [ 0.0085757  -0.00403203  0.00421548 ...  0.00241503 -0.00526051\n",
            "    0.00486947]\n",
            "  [ 0.00309052 -0.00114412  0.0051779  ...  0.00523721  0.00321043\n",
            "   -0.00491599]\n",
            "  ...\n",
            "  [-0.00998565  0.00675003 -0.00370846 ... -0.01447046  0.00797522\n",
            "    0.00393558]\n",
            "  [-0.01240476  0.0060449   0.00074663 ... -0.01742604  0.01215955\n",
            "    0.00414632]\n",
            "  [-0.0079066   0.00368942 -0.00148261 ... -0.01555973  0.01043784\n",
            "    0.0004095 ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.0005108  -0.00091487 -0.00076293 ... -0.00240077  0.00057624\n",
            "   -0.00174244]\n",
            "  [ 0.00153368  0.00196057  0.00300528 ...  0.00464465  0.00228364\n",
            "    0.00089973]\n",
            "  [ 0.00535451  0.00089247 -0.00473951 ...  0.00555759  0.0024509\n",
            "   -0.00306697]\n",
            "  ...\n",
            "  [ 0.00111467 -0.00160588 -0.00425066 ...  0.00302041  0.00162492\n",
            "    0.00322627]\n",
            "  [-0.00439492  0.00149852 -0.00333733 ...  0.00225315 -0.00479153\n",
            "   -0.00231855]\n",
            "  [-0.00312459  0.00424676 -0.00640296 ...  0.0030094   0.00143953\n",
            "   -0.00180795]]\n",
            "\n",
            " [[-0.0005108  -0.00091487 -0.00076293 ... -0.00240077  0.00057624\n",
            "   -0.00174244]\n",
            "  [-0.00468056 -0.00276033 -0.00081809 ... -0.0044156  -0.00268212\n",
            "    0.00086636]\n",
            "  [-0.00909451 -0.00013041 -0.00054253 ... -0.00244265 -0.00780284\n",
            "   -0.00452986]\n",
            "  ...\n",
            "  [-0.00359878 -0.00288578 -0.00641386 ... -0.01087161  0.00384455\n",
            "    0.00726452]\n",
            "  [-0.00791261 -0.00364907 -0.00810267 ... -0.01115161  0.0070239\n",
            "    0.00333067]\n",
            "  [-0.00940427 -0.00581662 -0.00741014 ... -0.01109283  0.00154642\n",
            "    0.00571239]]\n",
            "\n",
            " [[ 0.00180155  0.0031839   0.00166937 ...  0.00095975  0.00159403\n",
            "    0.00450479]\n",
            "  [ 0.00148072  0.0015607   0.00019473 ... -0.00194687  0.00109387\n",
            "    0.00148017]\n",
            "  [-0.00508955  0.00163754 -0.00246366 ... -0.00497972  0.00441821\n",
            "   -0.00262135]\n",
            "  ...\n",
            "  [ 0.00138473  0.00594465 -0.00973052 ... -0.00208124  0.00255011\n",
            "    0.00100059]\n",
            "  [-0.0002968   0.0070022  -0.0138196  ... -0.00110175  0.00159286\n",
            "    0.00106937]\n",
            "  [-0.00683712  0.00500814 -0.00481821 ... -0.00720276  0.00488752\n",
            "    0.00666639]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# 2d array of length 100 where each interior array is a prediction for the next character in the next timestep"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odasvJPVIahs",
        "outputId": "f23967b2-e944-493d-ce2e-149d81e0da2c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-0.00339666  0.00372586 -0.00232849 ... -0.00115589 -0.00019825\n",
            "   0.00094499]\n",
            " [-0.00316992  0.00132619 -0.00286747 ... -0.00390963 -0.00041807\n",
            "  -0.0013145 ]\n",
            " [-0.00281235  0.00308569 -0.00754597 ... -0.00070454  0.00506456\n",
            "  -0.00220535]\n",
            " ...\n",
            " [-0.00888389 -0.00183344 -0.00569069 ... -0.00555229  0.00339345\n",
            "  -0.00232633]\n",
            " [-0.00691008 -0.0031834  -0.00688785 ... -0.00659937  0.00247563\n",
            "  -0.00440373]\n",
            " [-0.00314979 -0.00036527 -0.00271043 ...  0.00134703  0.00321952\n",
            "  -0.00168075]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction for each time step\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# 65 values representing the prediction of the next character"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4McJSVAIe72",
        "outputId": "78e3b13d-f331-49c6-c88e-4b46ebae1768"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-3.3966561e-03  3.7258593e-03 -2.3284925e-03  4.6405208e-04\n",
            " -9.3733985e-04  1.5143754e-03  4.3055508e-04 -1.0022020e-03\n",
            " -6.2810159e-03 -6.5836776e-04  9.9644705e-04 -1.0627857e-03\n",
            " -4.2817257e-03 -2.3619509e-03  7.5291982e-03 -3.2232834e-03\n",
            "  1.3981643e-04 -3.7296412e-03 -1.8671958e-03  8.5892942e-04\n",
            "  5.2631581e-03  3.9782980e-04 -1.5964090e-03 -9.3981880e-04\n",
            "  6.7808293e-03 -6.6705421e-04 -3.3922987e-03  4.3701311e-04\n",
            " -4.7203787e-03  1.8988936e-03 -2.7058111e-03 -3.8031046e-03\n",
            "  8.2357507e-03  3.4296010e-03 -3.6744424e-04 -2.9932510e-03\n",
            "  5.3522801e-03 -1.5641218e-03  2.1864031e-04  8.5007574e-04\n",
            "  1.5418578e-03 -1.3240112e-03 -4.3200371e-03  7.6427604e-03\n",
            " -1.9367656e-03 -4.5194011e-03  4.6163648e-03  7.9570571e-05\n",
            " -2.2193128e-03  5.0714822e-03 -2.3162579e-03  3.0907956e-03\n",
            " -2.9080622e-03  2.5916239e-03 -8.8151929e-04 -4.9675425e-04\n",
            "  6.0751080e-04  8.5453130e-04  4.4859760e-03 -9.0702920e-04\n",
            " -2.1113958e-03  2.7016725e-04 -1.1558896e-03 -1.9824726e-04\n",
            "  9.4498752e-04], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# to determine the predicted character, we need to sample the output distribution(pick a character based on probability)\n",
        "sampled_indices =tf.random.categorical(pred, num_samples =1)\n",
        "# reshape the array and convert all integers to numbers\n",
        "sampled_indices = np.reshape(sampled_indices,(1, -1))[0]\n",
        "predicted_charactrs = int_to_text(sampled_indices)\n",
        "predicted_charactrs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JBeHXjqQIlX-",
        "outputId": "53c12abf-e1f0-4451-9583-b01ee5d81056"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"NNe$gM\\nx\\nTo?FspJwTgc&Mj;RiST,ZD:RK x'gnxX.RLfOAgEvQMSeCX Q;UgstHpMpZ? bYQ?ulZkpZqrtA:YgPtbb&OMNHqana\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# now we can create a loss funtion that compares that output to the expected output and gives us some numerical value telling how close the two were\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True) #logits is probability distribution(nodes(dense stuff)).....\n",
        "  # the goal of our algorithm in the network is to reduce the loss"
      ],
      "metadata": {
        "id": "lD3ZhIO9Il38"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "model.compile(optimizer= \"adam\", loss=loss)\n",
        "# its like a classification problem where the model predicts the probability of each unique letter coming next"
      ],
      "metadata": {
        "id": "Xq51JjPNIu34"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure our model to save checkpoints as it trains\n",
        "# allow us to load our model from a checkpoint to continue training\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir ='./training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n"
      ],
      "metadata": {
        "id": "GzNiS263IzaG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# training the model\n",
        "history = model.fit(data, epochs=2, callbacks=[checkpoint_callback] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VbmI2FFI2Qa",
        "outputId": "e69375aa-c5f5-435f-9cfd-8769b7882437"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "172/172 [==============================] - 11s 65ms/step - loss: 1.8836\n",
            "Epoch 2/2\n",
            "172/172 [==============================] - 12s 66ms/step - loss: 1.6493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rebuild the model using a batch size of 1 since its a prediction for 1 character not 64 characters(batches)\n",
        "model = build_model(VOCAB_SIZE, EMBDDING_DIM, RNN_UNIT, batch_size = 1)"
      ],
      "metadata": {
        "id": "n9kLQ7EkI5FS"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the latest checkpoint the stores the models weight\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "_zW3X4voI83c"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  num_generate = 400\n",
        "\n",
        "\n",
        "\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  # empty string to store result\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 1.2   #low temp - more predictable text, high temp - more suprising text\n",
        "\n",
        "# batch_size == 1\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # use a categorical distribution to predict the character from the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id =tf.randon.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # we pass the output(predicted charactr) from the model as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "j9quxaqPJDTR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inp = input('Type a starting string: ')\n",
        "print(generate_text(model,inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "Fc_F5TRUJQn5",
        "outputId": "c0196c9c-211e-4a91-e10e-eccf8b17e68d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type a starting string: rest\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-d1525a67302d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Type a starting string: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-599ed1264d2b>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, start_string)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# use a categorical distribution to predict the character from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mpredicted_id\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# we pass the output(predicted charactr) from the model as the next input to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'randon'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOPg+xHbO1qc6JjGlTk7IhZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}