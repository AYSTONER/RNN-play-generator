{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AYSTONER/RNN-play-generator/blob/main/RNN_play_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "9Z7crrt3M902",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82280ead-3b42-44b0-ab00-37f2dc3d011b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import keras\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "cs2ZaA2yOpO7"
      },
      "outputs": [],
      "source": [
        "# saving the path to it\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "YHRf2eREOqPH"
      },
      "outputs": [],
      "source": [
        "# to import your own file, use this code\n",
        "# from google.colab import files\n",
        "# path_to_file = list(files.upload().keys())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2vJO3HlPoBC",
        "outputId": "acc669c2-23c1-4cf7-d0c2-7c4e775832c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# read the file then decode\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# len of text is the amount of characters in it\n",
        "print(f'Length of text: {len(text)} characters')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "5UOqH9RUQmfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7044be7b-ca7c-4e90-dfe3-a1467a7dbdc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# first 250 characyers\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "CjRRkN-YRU24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31cc0e91-fe0f-4a2c-8a76-b21b47c4dfe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ],
      "source": [
        "# sort the unique characters in the data\n",
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "text_as_int=text_to_int(text)\n",
        "\n",
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "nEBiX0k-WTYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a29098e6-f998-4d42-92dd-b34af6e187dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ],
      "source": [
        "def int_to_text(ints):\n",
        "\n",
        "  try:\n",
        "\n",
        "    ints = ints.numpy()\n",
        "\n",
        "  except:\n",
        "\n",
        "    pass\n",
        "\n",
        "  return \"\".join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "w7fg7MU4W1mw"
      },
      "outputs": [],
      "source": [
        "# creating a training data\n",
        "seq_length = 100 #length of sequence for a training exmple\n",
        "num_per_epoch = len(text) // (seq_length + 1)\n",
        "# this code converts the entire string dataset into characters and will contain a stream of characters(training examples)\n",
        "character_dst = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "XcrM3zqjZKWl"
      },
      "outputs": [],
      "source": [
        "# use the batch method to batch the characters\n",
        "sequences = character_dst.batch(seq_length+1, drop_remainder=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "rpxddYiwo-wi"
      },
      "outputs": [],
      "source": [
        "# use the sequence of length 101 and split into input and output\n",
        "def split_input_target(chunk): # for the example: hello\n",
        "  input_text = chunk[:-1] #   hell\n",
        "  target_text = chunk [1:] # ello\n",
        "  return input_text, target_text # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target) #we use map to apply the above function to every entry\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "_2wNW54Kve38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5337db77-9f76-4d03-e685-0eef7b2adbf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n"
          ]
        }
      ],
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(x))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create our training batches\n",
        "BATCH_SIZE =64\n",
        "VOCAB_SIZE = len(vocab)#number of unique characters\n",
        "EMBDDING_DIM = 256\n",
        "RNN_UNIT = 1024\n",
        "\n",
        "BUFFER_SIZE = 1000\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder= True)\n"
      ],
      "metadata": {
        "id": "1JxaM8zdITFs"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# building the model\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                         recurrent_initializer='glorot_uniform'),\n",
        "  tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBDDING_DIM, RNN_UNIT, BATCH_SIZE)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkAliXgRIWJg",
        "outputId": "718f05ea-fa4e-4ffc-ce18-9daf7fb12f72"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_4 (Dense)             (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5330241 (20.33 MB)\n",
            "Trainable params: 5330241 (20.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a loss function\n",
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch) #ask our model for a prediction on our first batch of training data\n",
        "  print(example_batch_predictions.shape,\" (batch_size, sequence_length, vocab_size)\") #print out the output shape\n",
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)\n"
      ],
      "metadata": {
        "id": "Azi5JbR6IXCD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1c3db4-93b3-4100-a3ed-39c795d9d304"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65)  (batch_size, sequence_length, vocab_size)\n",
            "64\n",
            "tf.Tensor(\n",
            "[[[ 2.90187472e-03 -8.64573941e-03 -4.52754262e-04 ...  1.23450719e-03\n",
            "    5.52992336e-03  3.95592581e-03]\n",
            "  [-2.96954531e-03 -6.40117005e-03 -6.47847261e-03 ... -1.39941135e-03\n",
            "    2.17099092e-03  1.46700740e-02]\n",
            "  [-3.82969249e-03 -1.04468572e-03 -6.23628264e-03 ... -6.10565767e-03\n",
            "    1.37738278e-03  1.35648474e-02]\n",
            "  ...\n",
            "  [ 3.93566396e-03 -1.74706075e-02  5.05244639e-03 ...  1.56311877e-03\n",
            "    3.16285924e-03  4.98690689e-03]\n",
            "  [-2.32771016e-03 -1.60812885e-02 -2.76323408e-03 ... -1.40638882e-03\n",
            "    3.10819782e-03  7.80413486e-03]\n",
            "  [-2.61532236e-03 -1.53960213e-02  1.21810054e-03 ... -3.95171950e-03\n",
            "    3.23932222e-03  5.11337537e-03]]\n",
            "\n",
            " [[-1.62781368e-03  3.10006132e-03 -1.73694245e-03 ... -4.29182593e-03\n",
            "   -1.22540817e-03  2.41854251e-03]\n",
            "  [-6.12360984e-03  3.01651482e-04 -7.20979786e-03 ... -4.66218498e-03\n",
            "   -1.55302393e-03  5.21751028e-03]\n",
            "  [-2.40962091e-03  2.14562751e-03 -4.79089655e-03 ... -4.18087980e-03\n",
            "   -2.80290120e-03  6.21977029e-03]\n",
            "  ...\n",
            "  [-1.38723687e-03  1.55346608e-03  6.20148238e-03 ... -1.16236880e-02\n",
            "   -6.70450553e-03 -4.10887133e-03]\n",
            "  [-3.79664451e-03 -5.48061449e-03  9.84693179e-04 ... -9.93103441e-03\n",
            "   -1.41058387e-02 -8.63386691e-03]\n",
            "  [-2.51111388e-03 -6.51637511e-03  3.33925802e-03 ... -9.68218595e-03\n",
            "   -1.05655026e-02 -6.71234448e-03]]\n",
            "\n",
            " [[ 4.08028427e-04  2.38700095e-03 -1.07750832e-03 ... -4.23756428e-03\n",
            "    2.91362032e-03 -2.46531796e-03]\n",
            "  [ 2.41614250e-03 -3.33524263e-03 -2.06142175e-03 ... -5.58403227e-03\n",
            "    1.53082644e-03 -2.03239988e-03]\n",
            "  [-2.49251956e-04 -1.69587089e-03 -2.48318072e-03 ... -4.88790963e-03\n",
            "   -5.87152969e-03 -6.87754201e-03]\n",
            "  ...\n",
            "  [ 3.33197368e-03 -6.19930867e-03  3.27939610e-03 ... -6.06627529e-03\n",
            "   -7.22096814e-03 -5.60609298e-03]\n",
            "  [-4.05896280e-04 -3.49056697e-03  9.69805405e-04 ... -1.14406515e-02\n",
            "   -7.39395339e-03 -1.59353972e-03]\n",
            "  [ 7.98357651e-04 -1.00042624e-02  1.01185730e-03 ... -1.31936837e-02\n",
            "   -7.08167208e-03 -1.24470610e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 4.04512510e-03  4.28789575e-03 -1.26738870e-03 ... -3.59394215e-03\n",
            "   -4.16329317e-03 -1.43480231e-03]\n",
            "  [ 5.57324104e-03 -2.92655127e-03 -1.80190615e-03 ... -4.79319133e-03\n",
            "   -3.48269031e-03 -1.43514166e-03]\n",
            "  [ 6.39247568e-03 -7.96481315e-03 -2.75060651e-03 ... -4.80846781e-03\n",
            "   -3.53159476e-03 -1.76511414e-03]\n",
            "  ...\n",
            "  [ 6.10323623e-03 -5.80814295e-03  5.38910786e-03 ...  3.16146482e-03\n",
            "    1.99920940e-03  1.25706661e-03]\n",
            "  [ 7.21295830e-03 -1.23939998e-02  4.48028743e-03 ...  2.97232927e-03\n",
            "    7.03406427e-03  5.21377986e-03]\n",
            "  [ 1.23901041e-02 -3.88581585e-03  5.69368713e-03 ... -1.48468558e-03\n",
            "   -2.17083289e-04  6.64081238e-03]]\n",
            "\n",
            " [[ 9.32843518e-03  5.22675738e-03  1.88233028e-03 ... -3.23925633e-05\n",
            "   -7.83846481e-04  5.27271628e-03]\n",
            "  [ 1.55286910e-02  5.66046638e-03  6.48885826e-03 ...  7.94312079e-03\n",
            "    3.75035597e-05  5.86158037e-03]\n",
            "  [ 8.73186812e-03  6.29025372e-03  6.22468255e-03 ...  6.08498510e-03\n",
            "    1.48569176e-03  8.84028268e-04]\n",
            "  ...\n",
            "  [-8.28270987e-03 -3.01899528e-03 -1.03217363e-02 ... -1.29662100e-02\n",
            "   -8.76765698e-03  5.53445844e-03]\n",
            "  [-7.71732116e-03 -4.11519222e-03 -5.68566471e-03 ... -1.20675312e-02\n",
            "   -6.91559911e-03  4.15370520e-03]\n",
            "  [-5.61904907e-03 -8.41632485e-04 -5.83915925e-03 ... -1.09319547e-02\n",
            "   -6.25413842e-03  6.67776167e-03]]\n",
            "\n",
            " [[-1.03239319e-04 -1.72776054e-03  2.91364081e-03 ... -2.01186258e-03\n",
            "    1.00737589e-03 -8.97217076e-04]\n",
            "  [ 4.14343085e-05 -1.82493404e-03  3.18051176e-03 ... -4.94279433e-03\n",
            "   -2.01832227e-05 -2.91843666e-03]\n",
            "  [ 6.69800956e-03  3.19256447e-03  3.91217321e-03 ... -5.74854016e-03\n",
            "   -5.69860591e-03  6.81147911e-04]\n",
            "  ...\n",
            "  [-2.84218532e-03  6.09329902e-04  7.46332924e-04 ... -1.57699641e-02\n",
            "   -4.16852161e-03  4.63621691e-05]\n",
            "  [-7.53164012e-03  1.32684130e-03 -3.29471077e-04 ... -1.44510427e-02\n",
            "   -2.05989904e-03 -1.26118155e-03]\n",
            "  [-5.84192295e-03  4.61997464e-04 -7.23526347e-04 ... -1.63059831e-02\n",
            "   -3.49696586e-03 -2.23380397e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# 2d array of length 100 where each interior array is a prediction for the next character in the next timestep"
      ],
      "metadata": {
        "id": "odasvJPVIahs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629d7e63-e69c-4c6b-8e07-8cc452a51085"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[ 0.00290187 -0.00864574 -0.00045275 ...  0.00123451  0.00552992\n",
            "   0.00395593]\n",
            " [-0.00296955 -0.00640117 -0.00647847 ... -0.00139941  0.00217099\n",
            "   0.01467007]\n",
            " [-0.00382969 -0.00104469 -0.00623628 ... -0.00610566  0.00137738\n",
            "   0.01356485]\n",
            " ...\n",
            " [ 0.00393566 -0.01747061  0.00505245 ...  0.00156312  0.00316286\n",
            "   0.00498691]\n",
            " [-0.00232771 -0.01608129 -0.00276323 ... -0.00140639  0.0031082\n",
            "   0.00780413]\n",
            " [-0.00261532 -0.01539602  0.0012181  ... -0.00395172  0.00323932\n",
            "   0.00511338]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction for each time step\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# 65 values representing the prediction of the next character"
      ],
      "metadata": {
        "id": "k4McJSVAIe72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d0a8fb5-c2ab-49ea-8b43-a4ae91387e8f"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[ 2.9018747e-03 -8.6457394e-03 -4.5275426e-04 -5.9458390e-03\n",
            " -9.1681385e-04 -2.9490276e-03  2.2071507e-03  6.6788853e-03\n",
            " -3.9436994e-03  2.4298930e-03 -7.2926021e-05  3.8239933e-03\n",
            " -6.7915758e-03 -2.0783828e-03  2.3809441e-03  2.7900550e-04\n",
            " -3.2943422e-03 -1.1456059e-03 -5.4725595e-03 -7.1931951e-04\n",
            " -1.7413092e-03 -1.9141519e-03  1.9332453e-03  5.4377574e-04\n",
            "  4.8998250e-03 -4.1354490e-03  2.8137811e-03  2.8143581e-03\n",
            " -2.0140843e-03  6.1865719e-03  2.0247025e-03  7.0415414e-04\n",
            "  2.0874680e-03 -2.7035321e-03  6.7494088e-04 -2.0917563e-05\n",
            " -4.7043208e-03  2.6688618e-03 -4.4843783e-03  8.9718419e-04\n",
            "  7.7267044e-04  2.6209177e-03 -1.7208465e-03  1.5541710e-03\n",
            "  2.3786123e-03 -1.8487117e-04 -2.0960106e-03 -1.4579052e-03\n",
            " -3.9627813e-03 -8.6243656e-03 -6.0958527e-03 -8.8243658e-04\n",
            " -7.7046105e-03  1.3788685e-03 -1.2990828e-03 -1.1715367e-03\n",
            " -2.9979274e-04 -4.0294076e-03 -4.9001290e-03  9.4262656e-04\n",
            "  3.0622252e-03  9.1061322e-03  1.2345072e-03  5.5299234e-03\n",
            "  3.9559258e-03], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# to determine the predicted character, we need to sample the output distribution(pick a character based on probability)\n",
        "sampled_indices =tf.random.categorical(pred, num_samples =1)\n",
        "# reshape the array and convert all integers to numbers\n",
        "sampled_indices = np.reshape(sampled_indices,(1, -1))[0]\n",
        "predicted_charactrs = int_to_text(sampled_indices)\n",
        "predicted_charactrs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JBeHXjqQIlX-",
        "outputId": "4f75ad24-c2c9-46ae-be11-e85ae1cbe272"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"CwEmResbPOOABb-KLw-:BIl&Buz\\n\\n .JSgoH!KW-3A-M3!fJygXIb:H-rnxz\\nh KixjtD-lCb.:$&;n!vIL'AFDqQwp&C3Gb!Q?y\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# now we can create a loss funtion that compares that output to the expected output and gives us some numerical value telling how close the two were\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True) #logits is probability distribution(nodes(dense stuff)).....\n",
        "  # the goal of our algorithm in the network is to reduce the loss"
      ],
      "metadata": {
        "id": "lD3ZhIO9Il38"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "model.compile(optimizer= \"adam\", loss=loss)\n",
        "# its like a classification problem where the model predicts the probability of each unique letter coming next"
      ],
      "metadata": {
        "id": "Xq51JjPNIu34"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure our model to save checkpoints as it trains\n",
        "# allow us to load our model from a checkpoint to continue training\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir ='./training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n"
      ],
      "metadata": {
        "id": "GzNiS263IzaG"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# training the model\n",
        "history = model.fit(data, epochs=20, callbacks=[checkpoint_callback] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VbmI2FFI2Qa",
        "outputId": "18835b70-e8fa-4dbc-a0b8-88333d0369b8"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 16s 74ms/step - loss: 2.7831\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 13s 74ms/step - loss: 2.1033\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 13s 75ms/step - loss: 1.8631\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 13s 73ms/step - loss: 1.7028\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 70ms/step - loss: 1.5922\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.5147\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 1.4597\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 1.4171\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 1.3835\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 70ms/step - loss: 1.3552\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 1.3300\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 13s 74ms/step - loss: 1.3074\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.2860\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 1.2657\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.2465\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 1.2271\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 13s 73ms/step - loss: 1.2077\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.1871\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 1.1683\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.1482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rebuild the model using a batch size of 1 since its a prediction for 1 character not 64 characters(batches)\n",
        "model = build_model(VOCAB_SIZE, EMBDDING_DIM, RNN_UNIT, batch_size = 1)"
      ],
      "metadata": {
        "id": "n9kLQ7EkI5FS"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the latest checkpoint the stores the models weight\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "_zW3X4voI83c"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  num_generate = 400\n",
        "\n",
        "\n",
        "\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  # empty string to store result\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 1.2   #low temp - more predictable text, high temp - more suprising text\n",
        "\n",
        "# batch_size == 1\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # use a categorical distribution to predict the character from the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id =tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # we pass the output(predicted charactr) from the model as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "j9quxaqPJDTR"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inp = input('Type a starting string: ')\n",
        "print(generate_text(model,inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc_F5TRUJQn5",
        "outputId": "e2100138-2c9c-46db-b975-6ef375c28331"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: play\n",
            "play?\n",
            "\n",
            "Say was a foul word:\n",
            "Lie foot, mich opinior stil get no cause.\n",
            "\n",
            "HENRY BOWCKAMIO:\n",
            "KAil Silience!\n",
            "Tell her her wits, bride, take thee all Meremonius,\n",
            "Her ever hunning lead, who meautiling a months-sharps!\n",
            "Go above him, or mocking Puriedla!-with me to give incert!\n",
            "\n",
            "PETRUCHIO:\n",
            "Nay, take carry wed, being heldet\n",
            "To set inchised, hearing, to stain at\n",
            "any mirrow? there is sister,\n",
            "And my heart thee-buzz\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMnfXc9nl66NSXnebBKUjGb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}